{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.8.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import floor\n",
    "#import os\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "import time\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, ReLU, MaxPool1D, Dense, Dropout\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/yelp.csv\")\n",
    "train_df, test_val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "test_df, val_df = train_test_split(test_val_df,test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>review</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6oRAC4uyJCsJl1X0WZpVSA</td>\n",
       "      <td>2012-06-14</td>\n",
       "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>review</td>\n",
       "      <td>0hT2KtfLiobPvh6cDC8JQg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
       "      <td>2010-05-27</td>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars  \\\n",
       "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "2  6oRAC4uyJCsJl1X0WZpVSA  2012-06-14  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
       "3  _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4  6ozycU1RpktNG2-1BroVtw  2012-01-05  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "\n",
       "                                                text    type  \\\n",
       "0  My wife took me here on my birthday for breakf...  review   \n",
       "1  I have no idea why some people give bad review...  review   \n",
       "2  love the gyro plate. Rice is so good and I als...  review   \n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...  review   \n",
       "4  General Manager Scott Petello is a good egg!!!...  review   \n",
       "\n",
       "                  user_id  cool  useful  funny  \n",
       "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0  \n",
       "1  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0  \n",
       "2  0hT2KtfLiobPvh6cDC8JQg     0       1      0  \n",
       "3  uZetl9T0NcROGOyFfughhg     1       2      0  \n",
       "4  vYmM4KTsC8ZfQBg-j5MWkw     0       0      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"batch_size\": 128,\n",
    "    \"conv_channels\": 24,\n",
    "    \"conv_kernel_size\": 4,\n",
    "    \"conv_stride\": 2,\n",
    "    \"conv_padding\": 'valid',\n",
    "    \"conv_dilation\": 1,\n",
    "    \"embedding_dim\": 100,\n",
    "    \"dropout_rate\": 0.25,\n",
    "    \"output_size\": 1,\n",
    "    \"learning_rate\": 0.005,\n",
    "    \"max_num_words\": 10000,\n",
    "    \"max_sequence_length\": 250}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=hparams[\"max_num_words\"])\n",
    "tokenizer.fit_on_texts(train_df[\"text\"])\n",
    "\n",
    "def prep_text(texts, tokenizer, max_sequence_length):\n",
    "    # Turns text into into padded sequences.\n",
    "    for text in texts:\n",
    "        text_sequences = tokenizer.texts_to_sequences([text])\n",
    "        yield sequence.pad_sequences(\n",
    "            text_sequences, maxlen=max_sequence_length,\n",
    "            padding='post', truncating='post').reshape(-1)\n",
    "\n",
    "#text_train = lambda: prep_text(train_df[\"text\"], tokenizer, hparams[\"max_sequence_length\"])\n",
    "#text_test = lambda: prep_text(test_df[\"text\"], tokenizer, hparams[\"max_sequence_length\"])\n",
    "#text_val = lambda: prep_text(val_df[\"text\"], tokenizer, hparams[\"max_sequence_length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_callable(df, tokenizer, max_sequence_length, batch_size):\n",
    "    text_prepped = lambda: prep_text(df[\"text\"], tokenizer, max_sequence_length)\n",
    "    dataset = lambda: tf.data.Dataset.from_generator(\n",
    "        lambda: zip(text_prepped(),(i for i in df[\"stars\"])),\n",
    "        output_signature=(\n",
    "        tf.TensorSpec(shape=(max_sequence_length),dtype=tf.int32),\n",
    "        tf.TensorSpec(shape=(),dtype=tf.int32))).batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "train_dataset = dataset_callable(\n",
    "    train_df, tokenizer, hparams[\"max_sequence_length\"], hparams[\"batch_size\"])\n",
    "\n",
    "test_dataset = dataset_callable(\n",
    "    test_df, tokenizer, hparams[\"max_sequence_length\"], hparams[\"batch_size\"])\n",
    "\n",
    "val_dataset = dataset_callable(\n",
    "    val_df, tokenizer, hparams[\"max_sequence_length\"], hparams[\"batch_size\"])\n",
    "\n",
    "\n",
    "# train_dataset = lambda: tf.data.Dataset.from_generator(\n",
    "#     lambda: zip(text_train,(i for i in train_df[\"stars\"])),\n",
    "#     output_signature=(\n",
    "#         tf.TensorSpec(shape=(hparams[\"max_sequence_length\"]),dtype=tf.int32),\n",
    "#         tf.TensorSpec(shape=(),dtype=tf.int32))).batch(hparams[\"batch_size\"])\n",
    "\n",
    "# test_dataset = lambda: tf.data.Dataset.from_generator(\n",
    "#     lambda: zip(text_test,(i for i in test_df[\"stars\"])),\n",
    "#     output_signature=(\n",
    "#         tf.TensorSpec(shape=(hparams[\"max_sequence_length\"]),dtype=tf.int32),\n",
    "#         tf.TensorSpec(shape=(),dtype=tf.int32))).batch(hparams[\"batch_size\"])\n",
    "\n",
    "# val_dataset = lambda: tf.data.Dataset.from_generator(\n",
    "#     lambda: zip(text_val,(i for i in val_df[\"stars\"])),\n",
    "#     output_signature=(\n",
    "#         tf.TensorSpec(shape=(hparams[\"max_sequence_length\"]),dtype=tf.int32),\n",
    "#         tf.TensorSpec(shape=(),dtype=tf.int32))).batch(hparams[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTextCNN(Model):\n",
    "    def __init__(self,sequence_length=64,embedding_input_dim=1024,\n",
    "        embedding_dim=64, conv_channels=24,conv_kernel_size=4,conv_stride=2,\n",
    "        conv_padding='valid',conv_dilation=1, dropout_rate=0.25,\n",
    "        output_size=1):\n",
    "        super(SimpleTextCNN, self).__init__()\n",
    "        self.embedding = Embedding(\n",
    "            input_dim=embedding_input_dim,\n",
    "            output_dim=embedding_dim,\n",
    "            input_length=sequence_length)\n",
    "        self.conv1d = Conv1D(\n",
    "            filters=conv_channels,\n",
    "            kernel_size=conv_kernel_size,\n",
    "            strides=conv_stride,\n",
    "            padding=conv_padding,\n",
    "            data_format='channels_last',\n",
    "            dilation_rate=conv_dilation)\n",
    "        if conv_padding == 'valid':\n",
    "            self.conv_output_layer_size = floor(\n",
    "                (sequence_length - conv_dilation*(conv_kernel_size - 1) - 1) /\n",
    "                conv_stride + 1)\n",
    "        elif conv_padding == 'same':\n",
    "            self.conv_output_layer_size = floor(\n",
    "                (sequence_length - (conv_dilation-1)*(conv_kernel_size - 1) - 1) /\n",
    "                conv_stride + 1)\n",
    "        else:\n",
    "            raise(ValueError(f\"conv_padding must be one of 'valid' or 'same'\" +\n",
    "            f\"but received {conv_padding}\"))\n",
    "        self.relu = ReLU()\n",
    "        self.maxp1d = MaxPool1D(pool_size=self.conv_output_layer_size)\n",
    "        self.dropout = Dropout(rate=dropout_rate)\n",
    "        self.dense = Dense(units=output_size)\n",
    "\n",
    "    def call(self,x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.conv1d(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxp1d(x)\n",
    "        x = self.dropout(x)\n",
    "        x = tf.squeeze(x)\n",
    "        x = self.dense(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "model = SimpleTextCNN(\n",
    "  sequence_length=hparams[\"max_sequence_length\"],\n",
    "  embedding_input_dim=hparams[\"max_num_words\"],\n",
    "  embedding_dim=hparams[\"embedding_dim\"],\n",
    "  conv_channels=hparams[\"conv_channels\"],\n",
    "  conv_kernel_size=hparams[\"conv_kernel_size\"],\n",
    "  conv_stride=hparams[\"conv_stride\"],\n",
    "  conv_padding=hparams[\"conv_padding\"],\n",
    "  conv_dilation=hparams[\"conv_dilation\"],\n",
    "  dropout_rate=hparams[\"dropout_rate\"],\n",
    "  output_size=hparams[\"output_size\"])\n",
    "\n",
    "loss_object = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_rmse = tf.keras.metrics.RootMeanSquaredError(name='train_rmse')\n",
    "\n",
    "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "val_rmse = tf.keras.metrics.RootMeanSquaredError(name='val_rmse')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_rmse = tf.keras.metrics.RootMeanSquaredError(name='test_rmse')\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, features, targets, loss_object, optimizer,\n",
    "    train_loss, train_rmse):\n",
    "    with tf.GradientTape() as tape:\n",
    "    # training=True is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "        predictions = model(features, training=True)\n",
    "        loss = loss_object(targets, predictions)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_rmse(targets, predictions)\n",
    "\n",
    "@tf.function\n",
    "def test_step(model, features, targets, test_loss, test_rmse):\n",
    "  # training=False is only needed if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "    predictions = model(features, training=False)\n",
    "    t_loss = loss_object(targets, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_rmse(targets, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loss, train_dataset, train_rmse,\n",
    "    val_loss, val_dataset, val_rmse, epochs=5):\n",
    "    print(\"Start training...\\n\")\n",
    "    print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Train RMSE':^9} \" +\n",
    "          f\"| {'Val Loss':^10} | {'Val RMSE':^9} | {'Elapsed':^9}\")\n",
    "    print(\"-\"*60)\n",
    "    best_val_loss = None\n",
    "    for epoch in range(epochs):\n",
    "        t0_epoch = time.time()\n",
    "        train_loss.reset_state()\n",
    "        train_rmse.reset_state()\n",
    "        val_loss.reset_state()\n",
    "        val_rmse.reset_state()\n",
    "\n",
    "        for train_features, train_targets in train_dataset():\n",
    "            train_step(model, train_features, train_targets,\n",
    "            loss_object, optimizer, train_loss, train_rmse)\n",
    "\n",
    "        for val_features, val_targets in val_dataset():\n",
    "            test_step(model, val_features, val_targets, val_loss, val_rmse)\n",
    "\n",
    "        time_elapsed = time.time() - t0_epoch\n",
    "        print(f\"{epoch+1:^7} | {train_loss.result():^12.6f} | {train_rmse.result():^9.2f}\" +\n",
    "              f\" | {val_loss.result():^10.6f}\" +\n",
    "              f\" | {val_rmse.result():^9.2f} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "        if best_val_loss is not None and val_loss.result() > best_val_loss:\n",
    "            print(\"Stopping early: Val loss increased\")\n",
    "            break\n",
    "        else:\n",
    "            best_val_loss = val_loss.result()\n",
    "    print(f\"Training completed! Final validation RMSE: {val_rmse.result():.2f}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Train Loss  | Train RMSE |  Val Loss  | Val RMSE  |  Elapsed \n",
      "------------------------------------------------------------\n",
      "   1    |   7.919500   |   2.82    |  1.586187  |   1.26    |   3.32   \n",
      "   2    |   2.078290   |   1.44    |  1.455616  |   1.21    |   2.31   \n",
      "   3    |   1.873044   |   1.37    |  1.344856  |   1.16    |   2.34   \n",
      "   4    |   1.733359   |   1.32    |  1.246642  |   1.12    |   2.31   \n",
      "   5    |   1.535503   |   1.24    |  1.094809  |   1.04    |   2.32   \n",
      "   6    |   1.353235   |   1.16    |  0.987829  |   0.99    |   2.32   \n",
      "   7    |   1.197986   |   1.10    |  0.955442  |   0.98    |   2.36   \n",
      "   8    |   1.117679   |   1.06    |  0.937782  |   0.97    |   2.34   \n",
      "   9    |   1.042802   |   1.02    |  0.936548  |   0.97    |   2.37   \n",
      "  10    |   0.962515   |   0.98    |  0.931646  |   0.96    |   2.30   \n",
      "  11    |   0.918046   |   0.96    |  0.924252  |   0.96    |   2.47   \n",
      "  12    |   0.877753   |   0.94    |  0.914719  |   0.95    |   2.58   \n",
      "  13    |   0.851820   |   0.92    |  0.930068  |   0.96    |   2.56   \n",
      "Stopping early: Val loss increased\n",
      "Training completed! Final validation RMSE: 0.96.\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loss, train_dataset, train_rmse, val_loss, val_dataset, val_rmse, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\grego\\\\Anaconda3;C:\\\\Users\\\\grego\\\\Anaconda3\\\\Library\\\\mingw-w64\\\\bin;C:\\\\Users\\\\grego\\\\Anaconda3\\\\Library\\\\usr\\\\bin;C:\\\\Users\\\\grego\\\\Anaconda3\\\\Library\\\\bin;C:\\\\Users\\\\grego\\\\Anaconda3\\\\Scripts;C:\\\\Users\\\\grego\\\\Anaconda3\\\\bin;C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v11.6\\\\bin;C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v11.6\\\\libnvvp;C:\\\\Windows\\\\system32;C:\\\\Windows;C:\\\\Program Files\\\\Git\\\\cmd;C:\\\\Windows\\\\System32\\\\Wbem;C:\\\\Windows\\\\System32\\\\WindowsPowerShell\\\\v1.0;C:\\\\Windows\\\\System32\\\\OpenSSH;C:\\\\Program Files (x86)\\\\NVIDIA Corporation\\\\PhysX\\\\Common;C:\\\\Program Files\\\\Java\\\\jdk-14.0.1\\\\bin;C:\\\\Users\\\\grego\\\\AppData\\\\Local\\\\GitHubDesktop\\\\app-2.5.2\\\\resources\\\\app\\\\git\\\\cmd;C:\\\\Users\\\\grego\\\\AppData\\\\Local\\\\Programs\\\\MiKTeX 2.9\\\\miktex\\\\bin\\\\x64;C:\\\\Program Files\\\\Snowflake SnowSQL;C:\\\\Users\\\\grego\\\\Documents\\\\GitHub;C:\\\\Program Files\\\\maven\\\\apache-maven-3.6.3\\\\bin;C:\\\\Users\\\\grego\\\\Anaconda3;C:\\\\Users\\\\grego\\\\Anaconda3\\\\Library\\\\mingw-w64;C:\\\\Users\\\\grego\\\\Anaconda3\\\\Library\\\\bin;C:\\\\Users\\\\grego\\\\Anaconda3\\\\Scripts;C:\\\\Users\\\\grego\\\\Anaconda3\\\\condabin;C:\\\\Program Files\\\\NVIDIA Corporation\\\\NVIDIA NvDLISR;C:\\\\cudnn\\\\cuda\\\\bin;C:\\\\cudnnv7\\\\cuda\\\\bin;C:\\\\WINDOWS\\\\system32;C:\\\\WINDOWS;C:\\\\WINDOWS\\\\System32\\\\Wbem;C:\\\\WINDOWS\\\\System32\\\\WindowsPowerShell\\\\v1.0;C:\\\\WINDOWS\\\\System32\\\\OpenSSH;C:\\\\Program Files\\\\Graphviz\\\\bin;C:\\\\Program Files\\\\NVIDIA Corporation\\\\Nsight Compute 2022.1.0;C:\\\\Program Files\\\\NVIDIA\\\\CUDNN\\\\v8.3\\\\bin;C:\\\\Program Files\\\\NVIDIA\\\\CUDNN\\\\v8.3\\\\include;C:\\\\Program Files\\\\NVIDIA\\\\CUDNN\\\\v8.3\\\\lib\\\\x64;C:\\\\Program Files\\\\NVIDIA\\\\CUDNN\\\\v8.3;C:\\\\Users\\\\grego\\\\AppData\\\\Local\\\\Microsoft\\\\WindowsApps;C:\\\\Users\\\\grego\\\\AppData\\\\Local\\\\GitHubDesktop\\\\bin;C:\\\\Users\\\\grego\\\\AppData\\\\Local\\\\Programs\\\\MiKTeX 2.9\\\\miktex\\\\bin\\\\x64;C:\\\\Users\\\\grego\\\\AppData\\\\Local\\\\Programs\\\\Microsoft VS Code\\\\bin;C:\\\\Users\\\\grego\\\\Anaconda3;C:\\\\Users\\\\grego\\\\AppData\\\\Local\\\\GitHubDesktop\\\\app-2.6.0\\\\resources\\\\app\\\\git\\\\cmd;C:\\\\Users\\\\grego\\\\AppData\\\\Local\\\\Microsoft\\\\WindowsApps;C:\\\\Users\\\\grego\\\\Anaconda3\\\\lib\\\\site-packages\\\\numpy\\\\.libs;C:\\\\Users\\\\grego\\\\Anaconda3\\\\lib\\\\site-packages\\\\scipy\\\\.libs'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"PATH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bc7f50390e42f6c1331f86e9f26dea63ec7b34656187ee4894a92f7c1595689f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
